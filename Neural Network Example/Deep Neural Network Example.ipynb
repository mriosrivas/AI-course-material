{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Implementation\n",
    "The following Jupyter Notebook is an implementation of a Deep Neural Network using Python. The architecture that we will use is as follows:\n",
    "\n",
    "![Neural Network Architecture](nn_full.svg)\n",
    "\n",
    "We will have two inputs, and two hidden layers of 5 neurons each with a ReLu activation function, at the output we will have just one neuron with a sigmoid activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load library dependencies\n",
    "First we load our library dependencies, in this case we use three python libraries:\n",
    " * NumPy\n",
    " * Scikit-learn\n",
    " * Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [moons dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) which is a simple toy dataset to visualize clustering and classification algorithms with the `get_dataset` function from the `dataset` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 123\n",
    "# Get training and test data from moons dataset\n",
    "X, Y, X_val, Y_val, X_test, Y_test = get_dataset(random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass Functions\n",
    "In this part we will define the forward pass functions needed for our neural network.\n",
    "\n",
    "As a first step, we need to create our two activation functions:\n",
    "* Sigmoid\n",
    "$$f(x) = \\frac{1}{1+e^{-x}}$$\n",
    "* ReLu\n",
    "$$f(x) = \\max{(0, x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    ## START WRITING CODE HERE\n",
    "\n",
    "    ## FINISH WRITING CODE\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    ## START WRITING CODE HERE\n",
    "\n",
    "    ## FINISH WRITING CODE\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the `linear_forward` function, which performs the following calculation in the forward pass:\n",
    "$$ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter\n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ## START WRITING CODE HERE\n",
    "\n",
    "    ## FINISH WRITING CODE\n",
    "    \n",
    "    assert (Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `linear_activation_forward` function performs the following two steps:\n",
    " * the `linear_forward` calculation and then, \n",
    " * the activation function calculation, which can be a ReLu or sigmoid in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation\n",
    "    to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value\n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ## START WRITING CODE HERE\n",
    "\n",
    "        \n",
    "        ## FINISH WRITING CODE\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ## START WRITING CODE HERE\n",
    "\n",
    "        \n",
    "        ## FINISH WRITING CODE\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for the forward pass, we need to define the cost function. Our cost function is given by the [Cross Entropy Loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression), therefore remember to average your result at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by\n",
    "    https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from AL and Y.\n",
    "    ## START WRITING CODE HERE\n",
    "\n",
    "    ## FINISH WRITING CODE\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass Functions\n",
    "For the backward propagation part, we first need to define the derivatives for both the sigmoid and ReLu functions, and with this implement the backward propagation of the input `dA` to obtain the output `dZ`.\n",
    "\n",
    "The equations for de derivatives are as follows:\n",
    "* Sigmoid\n",
    "$$ \\frac{\\partial f(x)}{\\partial x} = \\frac{1}{1+e^{-x}}\\left(1-\\frac{1}{1+e^{-x}}\\right)$$\n",
    "\n",
    "* ReLu\n",
    "$$ \\frac{\\partial f(x)}{\\partial x} = \\begin{cases}\n",
    "        0 & \\text{if } x \\lt 0\\\\\n",
    "        1 & \\text{if } x \\geq 0\n",
    "    \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    ## START WRITING CODE HERE\n",
    "\n",
    "    \n",
    "    ## FINISH WRITING CODE\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well.\n",
    "    ## START WRITING CODE HERE\n",
    "\n",
    "    ## FINISH WRITING CODE\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `linear_backward` function, you will need to implement the linear portion of backward propagation for a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ## START WRITING CODE HERE\n",
    "\n",
    "    \n",
    "    ## FINISH WRITING CODE\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `linear_activation_backward` you will implement the backward pass of linear layer and the derivative of the activation function. Depending on the activation function, you will need to implement different pieces of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l\n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        ## START WRITING CODE HERE\n",
    "\n",
    "        \n",
    "        ## FINISH WRITING CODE\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        ## START WRITING CODE HERE\n",
    "\n",
    "        \n",
    "        ## FINISH WRITING CODE\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you will have to create a function called `cost_gradient` that will calculate the gradient of the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_gradient(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the gradient of the cost function defined \n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    dAL -- gradient of cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    ## START WRITING CODE HERE\n",
    "\n",
    "    ## FINISH WRITING CODE\n",
    "    return dAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters\n",
    "We will implement an auxiliary function called `initialize_parameters_deep` which will take a list of values containing the dimensions of each layer in our network. This function will return a dictionary of `parameters` where each key is either a weight or a bias initialized accordingly to a random value or zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # number of layers in the network\n",
    "\n",
    "    ## START WRITING CODE HERE\n",
    "\n",
    "    \n",
    "    ## FINISH WRITING CODE\n",
    "\n",
    "        assert (parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert (parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all Together\n",
    "After defining all the necesary blocks to perform forward and backward pass, we will implement three functions:\n",
    "* `forward_propagation` which will calculate the forward pass of our neural network model\n",
    "* `backward_propagation` which will be in charge of performing the backward calculation to obtain the gradients\n",
    "* `update` which will update the new weights and bias according the the gradients\n",
    "\n",
    "A function `accuracy` is provided to evaluate our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, inference=True):\n",
    "    ## START WRITING CODE HERE\n",
    "\n",
    "    \n",
    "    ## FINISH WRITING CODE\n",
    "    if(inference==True):\n",
    "        return AL\n",
    "    else:\n",
    "        return AL, cache1, cache2, cache3\n",
    "\n",
    "\n",
    "def backward_propagation(dAL, cache1, cache2, cache3):\n",
    "    ## START WRITING CODE HERE\n",
    "\n",
    "    ## FINISH WRITING CODE\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def update(parameters, gradients, learning_rate):\n",
    "    ## START WRITING CODE HERE\n",
    "\n",
    "    ## FINISH WRITING CODE\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def accuracy(Y_pred, Y, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Function that calculates the accuracy of our model.\n",
    "    \n",
    "    Arguments:\n",
    "    Y_pred -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "    threshold -- float number that sets the limit to select between true and false in our prediction. \n",
    "\n",
    "    Returns:\n",
    "    accuracy -- ratio between correct and total samples\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    total_samples = Y.shape[1]\n",
    "    correct = np.sum((Y_pred > threshold).astype('int') == Y)\n",
    "    incorrect = total_samples - correct\n",
    "    accuracy = correct / total_samples\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your Neural Network\n",
    "Now you will implement your neural network. You can try to train it with different `epochs` and `learning_rate` values. You can also try to change the dimensions of your network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "\n",
    "# Initialize parameters\n",
    "\n",
    "\n",
    "# Keep track of accuracy and cost\n",
    "epoch_train_accuracy = []\n",
    "epoch_val_accuracy = []\n",
    "total_cost = []\n",
    "\n",
    "\n",
    "for j in range(epochs):\n",
    "\n",
    "    ####### FORWARD PASS\n",
    "    # Linear activations\n",
    "\n",
    "    \n",
    "    # Compute cost\n",
    "\n",
    "    \n",
    "    ####### BACKWARD PASS\n",
    "    # Gradient of cost with respect of AL\n",
    "\n",
    "    \n",
    "    # Backpropagation\n",
    "\n",
    "    \n",
    "    ####### UPDATE\n",
    "\n",
    "    \n",
    "    \n",
    "    ####### EVALUATE MODEL        \n",
    "    # Test results on every iteration on training data\n",
    "\n",
    "    \n",
    "    \n",
    "    # Test results on every iteration on validation data\n",
    "\n",
    "    \n",
    "\n",
    "    ####### TRACK COST\n",
    "    # Calculate cost on every iteration\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_cost)\n",
    "plt.title('Cost vs. epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of our Results\n",
    "In this part you will see the error analysis on our train and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "train_error = 1 - np.array(epoch_train_accuracy)\n",
    "val_error = 1 - np.array(epoch_val_accuracy)\n",
    "print('\\nTraining information: ')\n",
    "print('Min Error = {:.2f}%'.format(100 * train_error.min()))\n",
    "print('Max Error = {:.2f}%'.format(100 * train_error.max()))\n",
    "\n",
    "plt.plot(train_error * 100, label='train error')\n",
    "plt.plot(val_error * 100, label='validation error')\n",
    "plt.title('Error vs Training Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.grid('on')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference on test data\n",
    "AL = forward_propagation(X_test, parameters)\n",
    "test_accuracy = accuracy(AL, Y_test)\n",
    "\n",
    "#correct_test, incorrect_test, epoch_inference_test, AL_test = inference(X_test, Y_test, parameters)\n",
    "error_test = 1 - np.array(test_accuracy)\n",
    "print('\\nInference on test data: ')\n",
    "print('Error = {:.2f}%'.format(100 * error_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
